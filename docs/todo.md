# TODO

## Improvements

- Support version in the inference as optional, if not provided, then get the last model version.
- Improve API to handle pagination to avoid request freezing for big data packages.
- Implement a low latency inference using Kafka or Redis Queue.