# Time Series Anomaly Detection Pipeline

This project provides a simple API for univariate time-series anomaly detection with support for multiple series registration, model versioning, artifact persistence, and predictions, designed as a clean and testable pipeline.

**Main frameworks and tools used in this project**:

> - [FastAPI](https://fastapi.tiangolo.com/): HTTP API routes and request and response handling.
> - [Pydantic](https://docs.pydantic.dev/): schema validation and data parsing.
> - [SQLAlchemy](https://www.sqlalchemy.org/): ORM and database access layer.
> - [PostgreSQL](https://www.postgresql.org/): metadata and model version persistence.
> - [Redis](https://redis.io/): latency telemetry storage
> - [NumPy](https://numpy.org/): baseline anomaly model statistics
> - [Plotly](https://plotly.com/python/): `/plot` HTML visualization of training data.
> - [Alembic](https://alembic.sqlalchemy.org/): database schema migrations.

## üõ†Ô∏è Setup

1. Clone and access the repository:

```bash
git clone git@github.com:lucasmazzetto/time_series_anomaly_detection_pipelineine.git

cd time_series_anomaly_detection_pipelineine
```

2. Create `.env` from `.env.example`:

```bash
mv .env.example .env
```

3. Edit `.env` as needed.  
   If you only want to run locally for testing purposes, defaults are enough.

## üê≥ Docker

Build and start all containers:

```bash
docker compose up --build
```

Stop containers:

```bash
docker compose down
```

> [!NOTE]
> This `docker-compose.yml` is designed for local development: it builds
> the API image from `Dockerfile`, pulls PostgreSQL and Redis images, and
> wires all services for local execution. If PostgreSQL and Redis are hosted
> in distributed or managed servers, you can split or adapt the compose file
> and point the API to external services.

## üöÄ API

Main API flow:

1. Train a series with `POST /fit/{series_id}`
2. Predict anomalies with `POST /predict/{series_id}` (optional query string: `?version=<number>`; default is latest)
3. Check service and metrics with `GET /healthcheck`
4. Visualize training data with `GET /plot?series_id=<id>[&version=<number>]` (`series_id` query is required, `version` is optional; open this endpoint in a browser to see the rendered chart)

### Example usage

#### Train:

To call `POST /fit/{series_id}`, provide a valid `series_id` in the path and a JSON body with `timestamps` and `values` arrays of the same length, ordered by time. The request must contain at least the minimum number of training points configured by `MIN_TRAINING_DATA_POINTS`.

```bash
curl -X POST http://localhost:8000/fit/sensor_01 \
  -H "Content-Type: application/json" \
  -d '{"timestamps":[1705000000,1705000001,1705000002,1705000003],"values":[10.0,11.2,10.4,10.8]}'
```

#### Predict (latest model):

To call `POST /predict/{series_id}` using the most recent trained model, provide the `series_id` in the path and send a JSON body with `timestamp` and `value`. If `version` is not provided, the API resolves the latest available version for that series.

```bash
curl -X POST http://localhost:8000/predict/sensor_01 \
  -H "Content-Type: application/json" \
  -d '{"timestamp":"1705000010","value":13.0}'
```

#### Predict (specific version):

To call the predict api with a model version, use the same endpoint and payload but add the query string `?version=<version>` (for example `?version=v1`). This is useful for reproducible tests and comparisons.

```bash
curl -X POST "http://localhost:8000/predict/sensor_01?version=v1" \
  -H "Content-Type: application/json" \
  -d '{"timestamp":"1705000011","value":10.3}'
```

#### Healthcheck:

`GET /healthcheck` returns a quick operational snapshot of the service, including the number of trained series and latency metrics for inference and training paths. The `series_trained` value counts unique `series_id`, not model versions, so a series with multiple versions is still counted once.

```bash
curl http://localhost:8000/healthcheck
```

## üìö Interactive docs:

These interactive docs are automatically generated by FastAPI from the API routes, request/response schemas, and OpenAPI metadata. They are useful for exploring endpoints, validating payloads, and testing requests directly in the browser.

- Swagger UI: `http://localhost:8000/docs`
- OpenAPI JSON: `http://localhost:8000/openapi.json`

## üß™ Testing

The test suite is isolated and does not persist data to your real database instance. API and database interactions are mocked or injected with dummy sessions, and file persistence tests use temporary paths, so running tests does not pollute runtime data.

Run all tests inside the API container:

```bash
docker compose exec api pytest -q
```

## üèõÔ∏è Architecture

This project uses a layered architecture: API routes handle HTTP contracts and validation, services orchestrate use cases, core components implement model and trainer logic, and database and storage layers manage PostgreSQL, Redis, and artifacts.

Detailed architecture and flow documentation:

- Architecture: [docs/architecture.md](docs/architecture.md)
- Training sequence: [docs/training-sequence.mmd](docs/training-sequence.mmd)
- Fit sequence: [docs/fit-sequence.mmd](docs/fit-sequence.mmd)
- Healthcheck sequence: [docs/healthcheck-sequence.mmd](docs/healthcheck-sequence.mmd)
- Plot flow: [docs/plot-view-flow.mmd](docs/plot-view-flow.mmd)

## ‚è±Ô∏è Benchmark

This project includes a stress benchmark jupyter notebook.
The benchmark is designed to evaluate API behavior under concurrent load for both training (`POST /fit/{series_id}`) and inference (`POST /predict/{series_id}`).

It computes endpoint metrics such as total calls, success and error counts, latency statistics (`min`, `p50`, `p95`, `max`, `mean` in ms), total wall-clock execution time, with histogram and box-plot visualizations.

**Summary usage:**

1. Start the stack (`docker compose up --build`)
2. Create and activate a virtual environment on your host machine:

```bash
python3 -m venv .venv
source .venv/bin/activate
```

3. Install notebook dependencies on your host machine:

```bash
pip install -r benchmark/requirements.txt
```

4. Open the notebook on your host machine (outside Docker)
5. Set `BASE_URL` (usually `http://localhost:8000`)
6. Run all cells

Full instructions at: [benchmark/README.md](benchmark/README.md)

## ‚öôÔ∏è Configuration

This project is configurable through environment variables, so you can adapt database connections, telemetry behavior, training limits, and storage paths without changing application code. The defaults below come from `.env.example` and are suitable for local development.

Default environment variables from `.env.example`:

| Variable | Default | Purpose |
|---|---|---|
| `DATABASE_URL` | `postgresql+psycopg2://postgres:postgres@db:5432/postgres` | SQLAlchemy database connection |
| `DATABASE_HOST` | `db` | PostgreSQL host (if not using explicit `DATABASE_URL`) |
| `DATABASE_PORT` | `5432` | PostgreSQL port |
| `DATABASE_NAME` | `postgres` | PostgreSQL database name |
| `DATABASE_USER` | `postgres` | PostgreSQL username |
| `DATABASE_PASSWORD` | `postgres` | PostgreSQL password |
| `REDIS_URL` | `redis://redis:6379/0` | Redis connection URL |
| `LATENCY_HISTORY_LIMIT` | `100` | Max latency samples stored per route type |
| `MIN_TRAINING_DATA_POINTS` | `3` | Minimum points required for training |
| `MODEL_STATE_FOLDER` | `./data/models` | Folder for persisted model states |
| `MODEL_FOLDER` | `./data/models` | Backward-compatible alias for model folder |
| `TRAINING_DATA_FOLDER` | `./data/data` | Folder for persisted training datasets |
| `DATA_FOLDER` | `./data/data` | Backward-compatible alias for data folder |

## üìà Scalability

To scale this project, run multiple stateless API instances behind a reverse proxy/load balancer (for example NGINX, Traefik, or a cloud LB) and distribute traffic across replicas. 

Keep persistence services shared: connect all API instances to central PostgreSQL (metadata/model versions) and Redis (telemetry/cache), ideally with managed/high-availability setups.

Artifacts and training data must also be shared across instances. One option is mounting `MODEL_STATE_FOLDER` and `TRAINING_DATA_FOLDER` on a Network File System (NFS) so every replica reads/writes the same storage. Another option is implementing a bucket-backed storage provider (for example AWS S3) that follows the existing storage interface, removing dependency on local disk and improving elasticity.

For production operation, also consider:

- running Alembic migrations as a one-time job per deployment
- autoscaling API replicas based on CPU/latency
- setting health/readiness checks at the load balancer

## üèÅ Roadmap

The next improvements required to make this project more robust are documented in the roadmap and are currently on the radar:

- Roadmap: [docs/roadmap.md](docs/roadmap.md)
